{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Using \"More preprocessed\" Data to run models on \"All Speakers\" and \"Main Characters\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- More preprocessed involves lemmatization and breaking down contractions\n",
    "- Main characters (4) : Cartman, Butters, Kyle, Stan (excluded Randy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semi pre-cleaned dataset\n",
    "df = pd.read_csv('cleaned_all-seasons.csv')\n",
    "\n",
    "#creating new colummn is_cartman\n",
    "df['is_cartman'] = 0\n",
    "df.loc[df.Character == 'Cartman', 'is_cartman'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuation\n",
    "import re, string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "corpus = df.Line.tolist()\n",
    "\n",
    "for line in range(len(corpus)):\n",
    "    corpus[line] = re.sub('\\\\n', '', corpus[line].rstrip()).lower()\n",
    "    corpus[line] = \" \".join(word.strip(string.punctuation) for word in corpus[line].split())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizing\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_lines(line):\n",
    "    word_list = word_tokenize(line)\n",
    "    word_list = [lem.lemmatize(w, pos='v') for w in word_list]\n",
    "    lem_line = ' '.join([lem.lemmatize(w) for w in word_list])\n",
    "    \n",
    "    return lem_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in range(len(corpus)):\n",
    "    corpus[line] = lemmatize_lines(corpus[line])\n",
    "    \n",
    "#corpus[6:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional stopwords - found from a kernal\n",
    "sw = ['be', 'you', 'i', 'to', 'the', 'do', 'it',\\\n",
    "        'a', 'we', 'that', 'and', 'have', 'go', 'what',\\\n",
    "        'get', 'of', 'this', 'in', 'on', 'all', 'just',\\\n",
    "        'for', 'he', 'know', 'will', 'but', 'with', 'so',\\\n",
    "        'they', 'now', 'well', \"'s\", 'guy', 'u', 'come',\\\n",
    "        'like', 'there', 'at', 'would', 'who', 'him',\\\n",
    "        'them', 'his', 'thing', 'where', 'should', 'an',\\\n",
    "        'please', 'maybe', 'their', 'even', 'any', 'than']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center>Using All Speakers</center>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect = TfidfVectorizer(stop_words=sw, ngram_range=(1,1))\n",
    "\n",
    "X = tf_vect.fit_transform(corpus)\n",
    "y = df.is_cartman\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Naive Bayes </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93     42785\n",
      "           1       0.92      0.01      0.03      6842\n",
      "\n",
      "    accuracy                           0.86     49627\n",
      "   macro avg       0.89      0.51      0.48     49627\n",
      "weighted avg       0.87      0.86      0.80     49627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "cv_scores = cross_val_score(mnb, X_train, y_train, cv=10)\n",
    "params = {'alpha': [1.0, 1.25, 1.35, 1.4]}\n",
    "nb_grid = GridSearchCV(mnb, params, cv=5) #using grid search for hyperparameter tuning\n",
    "\n",
    "nb_grid.fit(X_train, y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, nb_grid.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93     18337\n",
      "           1       0.50      0.01      0.01      2932\n",
      "\n",
      "    accuracy                           0.86     21269\n",
      "   macro avg       0.68      0.50      0.47     21269\n",
      "weighted avg       0.81      0.86      0.80     21269\n",
      "\n",
      "Recall: 0.007162346521145975\n"
     ]
    }
   ],
   "source": [
    "#tesing \n",
    "print(classification_report(y_test, nb_grid.predict(X_test),np.unique(y))) \n",
    "y_pred = nb_grid.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Decision Tree </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.78      0.83     42785\n",
      "           1       0.20      0.35      0.26      6842\n",
      "\n",
      "    accuracy                           0.72     49627\n",
      "   macro avg       0.54      0.57      0.54     49627\n",
      "weighted avg       0.79      0.72      0.75     49627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtree = tree.DecisionTreeClassifier(#max_depth=6,\n",
    "    class_weight=\"balanced\",\n",
    "    min_weight_fraction_leaf=0.03)\n",
    "dtree = dtree.fit(X_train,y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, dtree.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.78      0.83     18337\n",
      "           1       0.19      0.33      0.25      2932\n",
      "\n",
      "    accuracy                           0.72     21269\n",
      "   macro avg       0.54      0.56      0.54     21269\n",
      "weighted avg       0.79      0.72      0.75     21269\n",
      "\n",
      "Recall: 0.33321964529331516\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, dtree.predict(X_test),np.unique(y))) #testing\n",
    "y_pred = dtree.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Random Forest </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77     12166\n",
      "           1       0.60      0.57      0.58      6842\n",
      "\n",
      "    accuracy                           0.71     19008\n",
      "   macro avg       0.68      0.68      0.68     19008\n",
      "weighted avg       0.70      0.71      0.70     19008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400,\n",
    "                            max_features='sqrt',\n",
    "                            class_weight='balanced',\n",
    "                            max_depth=5,\n",
    "                            oob_score=True,\n",
    "                            random_state=3,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, rf.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.81      0.86     18337\n",
      "           1       0.29      0.48      0.36      2932\n",
      "\n",
      "    accuracy                           0.76     21269\n",
      "   macro avg       0.60      0.64      0.61     21269\n",
      "weighted avg       0.82      0.76      0.79     21269\n",
      "\n",
      "Recall: 0.4771487039563438\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, rf.predict(X_test), np.unique(y))) \n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> SVM - Balanced </u>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.75      0.80     12166\n",
      "           1       0.63      0.75      0.69      6842\n",
      "\n",
      "    accuracy                           0.75     19008\n",
      "   macro avg       0.74      0.75      0.74     19008\n",
      "weighted avg       0.77      0.75      0.76     19008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(C=0.05, max_iter=5000, class_weight='balanced', random_state=3)\n",
    "svm.fit(X_train,y_train)\n",
    "#training\n",
    "print(classification_report(y_train, svm.predict(X_train), np.unique(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.70      0.73      5215\n",
      "           1       0.54      0.63      0.58      2932\n",
      "\n",
      "    accuracy                           0.67      8147\n",
      "   macro avg       0.65      0.66      0.66      8147\n",
      "weighted avg       0.69      0.67      0.68      8147\n",
      "\n",
      "Recall: 0.6275579809004093\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, svm.predict(X_test), np.unique(y))) \n",
    "y_pred = svm.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> SVM - Unbalanced </u>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93     42785\n",
      "           1       0.57      0.24      0.34      6842\n",
      "\n",
      "    accuracy                           0.87     49627\n",
      "   macro avg       0.73      0.61      0.64     49627\n",
      "weighted avg       0.85      0.87      0.85     49627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(C=0.05, max_iter=5000, class_weight={0: 0.6, 1: 1.5}, random_state=3)\n",
    "svm.fit(X_train,y_train)\n",
    "#training\n",
    "print(classification_report(y_train, svm.predict(X_train), np.unique(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92     18337\n",
      "           1       0.49      0.19      0.27      2932\n",
      "\n",
      "    accuracy                           0.86     21269\n",
      "   macro avg       0.68      0.58      0.60     21269\n",
      "weighted avg       0.83      0.86      0.83     21269\n",
      "\n",
      "Recall: 0.18894952251023192\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, svm.predict(X_test), np.unique(y))) \n",
    "y_pred = svm.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Logistic Regression </u>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tatiksha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.80      0.87     42785\n",
      "           1       0.39      0.78      0.52      6842\n",
      "\n",
      "    accuracy                           0.80     49627\n",
      "   macro avg       0.67      0.79      0.69     49627\n",
      "weighted avg       0.88      0.80      0.82     49627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logis = LogisticRegression(class_weight = \"balanced\") \n",
    "logis.fit(X_train, y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, logis.predict(X_train), np.unique(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.78      0.85     18337\n",
      "           1       0.29      0.56      0.39      2932\n",
      "\n",
      "    accuracy                           0.75     21269\n",
      "   macro avg       0.61      0.67      0.62     21269\n",
      "weighted avg       0.83      0.75      0.78     21269\n",
      "\n",
      "Recall: 0.5648021828103683\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, logis.predict(X_test), np.unique(y))) \n",
    "y_pred = logis.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Ensemble Method - All Characters </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, precision_score, precision_recall_curve, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = VotingClassifier(estimators=[('LR',logis),('SVM', svm)], voting='hard',weights=[1,0.5])\n",
    "ensemble_fit= ensemble.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train/test F1 for Ensemble:  0.7731481481481481 0.7698539339634222\n",
      "Recall: 0.7598908594815825\n"
     ]
    }
   ],
   "source": [
    "ensemble_train_preds = ensemble.predict(X_train) #get predicted outputs of the classifier\n",
    "ensemble_train_f1 = f1_score(y_train, ensemble_train_preds, average='micro')\n",
    "ensemble_test_preds = ensemble.predict(X_test)\n",
    "ensemble_test_f1 = f1_score(y_test, ensemble_test_preds, average='micro')\n",
    "print(\"\\nTrain/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)\n",
    "\n",
    "print(\"Recall:\",metrics.recall_score(y_test, ensemble_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<center>* * *</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<center>Using Main Characters</center>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Butters' 'Cartman' 'Kyle' 'Stan']\n"
     ]
    }
   ],
   "source": [
    "top_speakers = df.groupby(['Character']).size().loc[df.groupby(['Character']).size() > 2500] \n",
    "top_speakers\n",
    "print(top_speakers.index.values)\n",
    "df1 = df.loc[df['Character'].isin(top_speakers.index.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuation\n",
    "corpus = df1.Line.tolist()\n",
    "\n",
    "for line in range(len(corpus)):\n",
    "    corpus[line] = re.sub('\\\\n', '', corpus[line].rstrip()).lower()\n",
    "    corpus[line] = \" \".join(word.strip(string.punctuation) for word in corpus[line].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizing\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_lines(line):\n",
    "    word_list = word_tokenize(line)\n",
    "    word_list = [lem.lemmatize(w, pos='v') for w in word_list]\n",
    "    lem_line = ' '.join([lem.lemmatize(w) for w in word_list])\n",
    "    \n",
    "    return lem_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in range(len(corpus)):\n",
    "    corpus[line] = lemmatize_lines(corpus[line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect = TfidfVectorizer(stop_words=sw, ngram_range=(1,1))\n",
    "\n",
    "X = tf_vect.fit_transform(corpus)\n",
    "y = df1.is_cartman\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Naive Bayes </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.99      0.84     12166\n",
      "           1       0.94      0.33      0.49      6842\n",
      "\n",
      "    accuracy                           0.75     19008\n",
      "   macro avg       0.83      0.66      0.66     19008\n",
      "weighted avg       0.80      0.75      0.71     19008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "cv_scores = cross_val_score(mnb, X_train, y_train, cv=10)\n",
    "params = {'alpha': [1.0, 1.25, 1.35, 1.4]}\n",
    "nb_grid = GridSearchCV(mnb, params, cv=5) #using grid search for hyperparameter tuning\n",
    "\n",
    "nb_grid.fit(X_train, y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, nb_grid.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.96      0.79      5215\n",
      "           1       0.71      0.19      0.30      2932\n",
      "\n",
      "    accuracy                           0.68      8147\n",
      "   macro avg       0.69      0.57      0.55      8147\n",
      "weighted avg       0.69      0.68      0.62      8147\n",
      "\n",
      "Recall: 0.18963165075034105\n"
     ]
    }
   ],
   "source": [
    "#tesing \n",
    "print(classification_report(y_test, nb_grid.predict(X_test),np.unique(y))) \n",
    "y_pred = nb_grid.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Decision Tree </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.79      0.74     12166\n",
      "           1       0.49      0.36      0.41      6842\n",
      "\n",
      "    accuracy                           0.63     19008\n",
      "   macro avg       0.59      0.57      0.57     19008\n",
      "weighted avg       0.62      0.63      0.62     19008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtree = tree.DecisionTreeClassifier(#max_depth=6,\n",
    "    class_weight=\"balanced\",\n",
    "    min_weight_fraction_leaf=0.03)\n",
    "dtree = dtree.fit(X_train,y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, dtree.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73      5215\n",
      "           1       0.48      0.35      0.41      2932\n",
      "\n",
      "    accuracy                           0.63      8147\n",
      "   macro avg       0.58      0.57      0.57      8147\n",
      "weighted avg       0.61      0.63      0.61      8147\n",
      "\n",
      "Recall: 0.3506139154160982\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, dtree.predict(X_test),np.unique(y))) #testing\n",
    "y_pred = dtree.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Random Forest </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77     12166\n",
      "           1       0.60      0.57      0.58      6842\n",
      "\n",
      "    accuracy                           0.71     19008\n",
      "   macro avg       0.68      0.68      0.68     19008\n",
      "weighted avg       0.70      0.71      0.70     19008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400,\n",
    "                            max_features='sqrt',\n",
    "                            class_weight='balanced',\n",
    "                            max_depth=5,\n",
    "                            oob_score=True,\n",
    "                            random_state=3,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, rf.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.76      0.75      5215\n",
      "           1       0.56      0.53      0.54      2932\n",
      "\n",
      "    accuracy                           0.68      8147\n",
      "   macro avg       0.65      0.65      0.65      8147\n",
      "weighted avg       0.68      0.68      0.68      8147\n",
      "\n",
      "Recall: 0.5313778990450204\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, rf.predict(X_test), np.unique(y))) \n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> SVM (Balanced) </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.75      0.80     12166\n",
      "           1       0.63      0.75      0.69      6842\n",
      "\n",
      "    accuracy                           0.75     19008\n",
      "   macro avg       0.74      0.75      0.74     19008\n",
      "weighted avg       0.77      0.75      0.76     19008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_bal = LinearSVC(C=0.05, max_iter=5000, class_weight='balanced', random_state=3)\n",
    "svm_bal.fit(X_train,y_train)\n",
    "#training\n",
    "print(classification_report(y_train, svm_bal.predict(X_train), np.unique(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.70      0.73      5215\n",
      "           1       0.54      0.63      0.58      2932\n",
      "\n",
      "    accuracy                           0.67      8147\n",
      "   macro avg       0.65      0.66      0.66      8147\n",
      "weighted avg       0.69      0.67      0.68      8147\n",
      "\n",
      "Recall: 0.6275579809004093\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, svm_bal.predict(X_test), np.unique(y))) \n",
    "y_pred = svm_bal.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> SVM (Unbalanced) </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.45      0.60     12166\n",
      "           1       0.48      0.91      0.63      6842\n",
      "\n",
      "    accuracy                           0.62     19008\n",
      "   macro avg       0.69      0.68      0.62     19008\n",
      "weighted avg       0.75      0.62      0.61     19008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(C=0.05, max_iter=5000, class_weight={0: 0.6, 1: 1.5}, random_state=3)\n",
    "svm.fit(X_train,y_train)\n",
    "#training\n",
    "print(classification_report(y_train, svm.predict(X_train), np.unique(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.40      0.54      5215\n",
      "           1       0.45      0.87      0.59      2932\n",
      "\n",
      "    accuracy                           0.57      8147\n",
      "   macro avg       0.64      0.63      0.56      8147\n",
      "weighted avg       0.70      0.57      0.56      8147\n",
      "\n",
      "Recall: 0.8656207366984994\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, svm.predict(X_test), np.unique(y))) \n",
    "y_pred = svm.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Logistic Regression </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tatiksha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.82     12166\n",
      "           1       0.66      0.77      0.71      6842\n",
      "\n",
      "    accuracy                           0.78     19008\n",
      "   macro avg       0.76      0.78      0.77     19008\n",
      "weighted avg       0.79      0.78      0.78     19008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logis = LogisticRegression(class_weight = \"balanced\") \n",
    "logis.fit(X_train, y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, logis.predict(X_train), np.unique(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74      5215\n",
      "           1       0.55      0.62      0.58      2932\n",
      "\n",
      "    accuracy                           0.68      8147\n",
      "   macro avg       0.66      0.67      0.66      8147\n",
      "weighted avg       0.69      0.68      0.68      8147\n",
      "\n",
      "Recall: 0.6193724420190996\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, logis.predict(X_test), np.unique(y))) \n",
    "y_pred = logis.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Ensemble Method - Main Characters </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train/test F1 for Ensemble:  0.7731481481481481 0.7698539339634222\n",
      "Recall: 0.7598908594815825\n"
     ]
    }
   ],
   "source": [
    "ensemble = VotingClassifier(estimators=[('LR',logis),('SVM -Unbalanced', svm)],voting='hard',weights=[0.5,0.5])\n",
    "ftted= ensemble.fit(X, y)\n",
    "ensemble_train_preds = ensemble.predict(X_train) #get predicted outputs of the classifier\n",
    "ensemble_train_f1 = f1_score(y_train, ensemble_train_preds, average='micro')\n",
    "ensemble_test_preds = ensemble.predict(X_test)\n",
    "ensemble_test_f1 = f1_score(y_test, ensemble_test_preds, average='micro')\n",
    "print(\"\\nTrain/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)\n",
    "\n",
    "print(\"Recall:\",metrics.recall_score(y_test, ensemble_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = VotingClassifier(estimators=[('LR',logis),('SVM -Unbalanced', svm)], voting='hard',weights=[0.5,1])\n",
    "ftted= ensemble.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train/test F1 for Ensemble:  0.6327335858585859 0.6350803976924021\n",
      "Recall: 0.9017735334242838\n",
      "Precision: 0.49615312441358606\n"
     ]
    }
   ],
   "source": [
    "ensemble_train_preds = ensemble.predict(X_train) #get predicted outputs of the classifier\n",
    "ensemble_train_f1 = f1_score(y_train, ensemble_train_preds, average='micro')\n",
    "ensemble_test_preds = ensemble.predict(X_test)\n",
    "ensemble_test_f1 = f1_score(y_test, ensemble_test_preds, average='micro')\n",
    "print(\"\\nTrain/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)\n",
    "\n",
    "print(\"Recall:\",metrics.recall_score(y_test, ensemble_test_preds))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, ensemble_test_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
