{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Using \"Less preprocessed\" Data to run models on \"All Speakers\" and \"Main Characters\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- less preprocessed here means excluding lemmatization, contractions\n",
    "- Main characters (4) : Cartman, Butters, Kyle, Stan (excluded Randy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semi pre-cleaned dataset\n",
    "df = pd.read_csv('cleaned_all-seasons.csv')\n",
    "\n",
    "#creating new colummn is_cartman\n",
    "df['is_cartman'] = 0\n",
    "df.loc[df.Character == 'Cartman', 'is_cartman'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuation\n",
    "import re, string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "corpus = df.Line.tolist()\n",
    "\n",
    "for line in range(len(corpus)):\n",
    "    corpus[line] = re.sub('\\\\n', '', corpus[line].rstrip()).lower()\n",
    "    corpus[line] = \" \".join(word.strip(string.punctuation) for word in corpus[line].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional stopwords\n",
    "sw = ['be', 'you', 'i', 'to', 'the', 'do', 'it',\\\n",
    "        'a', 'we', 'that', 'and', 'have', 'go', 'what',\\\n",
    "        'get', 'of', 'this', 'in', 'on', 'all', 'just',\\\n",
    "        'for', 'he', 'know', 'will', 'but', 'with', 'so',\\\n",
    "        'they', 'now', 'well', \"'s\", 'guy', 'u', 'come',\\\n",
    "        'like', 'there', 'at', 'would', 'who', 'him',\\\n",
    "        'them', 'his', 'thing', 'where', 'should', 'an',\\\n",
    "        'please', 'maybe', 'their', 'even', 'any', 'than']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center>Using All Speakers</center>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect = TfidfVectorizer(stop_words=sw, ngram_range=(1,1))\n",
    "\n",
    "X = tf_vect.fit_transform(corpus)\n",
    "y = df.is_cartman\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Naive Bayes </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93     42785\n",
      "           1       0.94      0.02      0.04      6842\n",
      "\n",
      "    accuracy                           0.86     49627\n",
      "   macro avg       0.90      0.51      0.48     49627\n",
      "weighted avg       0.88      0.86      0.81     49627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "cv_scores = cross_val_score(mnb, X_train, y_train, cv=10)\n",
    "params = {'alpha': [1.0, 1.25, 1.35, 1.4]}\n",
    "nb_grid = GridSearchCV(mnb, params, cv=5) #using grid search for hyperparameter tuning\n",
    "\n",
    "nb_grid.fit(X_train, y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, nb_grid.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93     18337\n",
      "           1       0.52      0.01      0.02      2932\n",
      "\n",
      "    accuracy                           0.86     21269\n",
      "   macro avg       0.69      0.50      0.47     21269\n",
      "weighted avg       0.82      0.86      0.80     21269\n",
      "\n",
      "Recall: 0.009890859481582538\n"
     ]
    }
   ],
   "source": [
    "#tesing \n",
    "print(classification_report(y_test, nb_grid.predict(X_test),np.unique(y))) \n",
    "y_pred = nb_grid.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Decision Tree </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.79      0.83     42785\n",
      "           1       0.22      0.38      0.28      6842\n",
      "\n",
      "    accuracy                           0.73     49627\n",
      "   macro avg       0.56      0.58      0.56     49627\n",
      "weighted avg       0.80      0.73      0.76     49627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtree = tree.DecisionTreeClassifier(#max_depth=6,\n",
    "    class_weight=\"balanced\",\n",
    "    min_weight_fraction_leaf=0.03)\n",
    "dtree = dtree.fit(X_train,y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, dtree.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.79      0.83     18337\n",
      "           1       0.22      0.38      0.28      2932\n",
      "\n",
      "    accuracy                           0.73     21269\n",
      "   macro avg       0.55      0.58      0.56     21269\n",
      "weighted avg       0.80      0.73      0.76     21269\n",
      "\n",
      "Recall: 0.37517053206002726\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, dtree.predict(X_test),np.unique(y))) #testing\n",
    "y_pred = dtree.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Random Forest </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.83      0.87     42785\n",
      "           1       0.33      0.53      0.41      6842\n",
      "\n",
      "    accuracy                           0.79     49627\n",
      "   macro avg       0.62      0.68      0.64     49627\n",
      "weighted avg       0.84      0.79      0.81     49627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400,\n",
    "                            max_features='sqrt',\n",
    "                            class_weight='balanced',\n",
    "                            max_depth=5,\n",
    "                            oob_score=True,\n",
    "                            random_state=3,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, rf.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.86     18337\n",
      "           1       0.30      0.49      0.37      2932\n",
      "\n",
      "    accuracy                           0.77     21269\n",
      "   macro avg       0.61      0.65      0.62     21269\n",
      "weighted avg       0.83      0.77      0.80     21269\n",
      "\n",
      "Recall: 0.4853342428376535\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, rf.predict(X_test), np.unique(y))) \n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> SVM - Balanced </u>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.88     42785\n",
      "           1       0.39      0.76      0.51      6842\n",
      "\n",
      "    accuracy                           0.80     49627\n",
      "   macro avg       0.67      0.78      0.69     49627\n",
      "weighted avg       0.88      0.80      0.83     49627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_bal = LinearSVC(C=0.05, max_iter=5000, class_weight='balanced', random_state=3)\n",
    "svm_bal.fit(X_train,y_train)\n",
    "#training\n",
    "print(classification_report(y_train, svm_bal.predict(X_train), np.unique(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.79      0.85     18337\n",
      "           1       0.31      0.57      0.40      2932\n",
      "\n",
      "    accuracy                           0.76     21269\n",
      "   macro avg       0.61      0.68      0.63     21269\n",
      "weighted avg       0.84      0.76      0.79     21269\n",
      "\n",
      "Recall: 0.5746930422919508\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, svm_bal.predict(X_test), np.unique(y))) \n",
    "y_pred = svm_bal.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> SVM - Unbalanced </u>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93     42785\n",
      "           1       0.58      0.26      0.36      6842\n",
      "\n",
      "    accuracy                           0.87     49627\n",
      "   macro avg       0.74      0.62      0.64     49627\n",
      "weighted avg       0.85      0.87      0.85     49627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(C=0.05, max_iter=5000, class_weight={0: 0.6, 1: 1.5}, random_state=3)\n",
    "svm.fit(X_train,y_train)\n",
    "#training\n",
    "print(classification_report(y_train, svm.predict(X_train), np.unique(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92     18337\n",
      "           1       0.49      0.21      0.30      2932\n",
      "\n",
      "    accuracy                           0.86     21269\n",
      "   macro avg       0.69      0.59      0.61     21269\n",
      "weighted avg       0.83      0.86      0.84     21269\n",
      "\n",
      "Recall: 0.21384720327421555\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, svm.predict(X_test), np.unique(y))) \n",
    "y_pred = svm.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Logistic Regression </u>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tatiksha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.88     42785\n",
      "           1       0.41      0.80      0.54      6842\n",
      "\n",
      "    accuracy                           0.81     49627\n",
      "   macro avg       0.69      0.81      0.71     49627\n",
      "weighted avg       0.89      0.81      0.84     49627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logis = LogisticRegression(class_weight = \"balanced\") \n",
    "logis.fit(X_train, y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, logis.predict(X_train), np.unique(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.80      0.85     18337\n",
      "           1       0.31      0.57      0.40      2932\n",
      "\n",
      "    accuracy                           0.77     21269\n",
      "   macro avg       0.61      0.68      0.63     21269\n",
      "weighted avg       0.84      0.77      0.79     21269\n",
      "\n",
      "Recall: 0.5678717598908595\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, logis.predict(X_test), np.unique(y))) \n",
    "y_pred = logis.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Ensemble Method - All Characters </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, precision_score, precision_recall_curve, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tatiksha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ensemble = VotingClassifier(estimators=[('LR',logis),('SVM', svm)], voting='hard',weights=[1,0.5])\n",
    "ensemble_fit= ensemble.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train/test F1 for Ensemble:  0.8072823261531022 0.8100521886313414\n",
      "Recall: 0.7800136425648022\n"
     ]
    }
   ],
   "source": [
    "ensemble_train_preds = ensemble.predict(X_train) #get predicted outputs of the classifier\n",
    "ensemble_train_f1 = f1_score(y_train, ensemble_train_preds, average='micro')\n",
    "ensemble_test_preds = ensemble.predict(X_test)\n",
    "ensemble_test_f1 = f1_score(y_test, ensemble_test_preds, average='micro')\n",
    "print(\"\\nTrain/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)\n",
    "\n",
    "print(\"Recall:\",metrics.recall_score(y_test, ensemble_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<center>* * *</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<center>Using Main Characters</center>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Butters' 'Cartman' 'Kyle' 'Stan']\n"
     ]
    }
   ],
   "source": [
    "top_speakers = df.groupby(['Character']).size().loc[df.groupby(['Character']).size() > 2500] \n",
    "top_speakers\n",
    "print(top_speakers.index.values)\n",
    "df1 = df.loc[df['Character'].isin(top_speakers.index.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuation\n",
    "corpus = df1.Line.tolist()\n",
    "\n",
    "for line in range(len(corpus)):\n",
    "    corpus[line] = re.sub('\\\\n', '', corpus[line].rstrip()).lower()\n",
    "    corpus[line] = \" \".join(word.strip(string.punctuation) for word in corpus[line].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect = TfidfVectorizer(stop_words=sw, ngram_range=(1,1))\n",
    "\n",
    "X = tf_vect.fit_transform(corpus)\n",
    "y = df1.is_cartman\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Naive Bayes </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.99      0.84     12166\n",
      "           1       0.95      0.33      0.49      6842\n",
      "\n",
      "    accuracy                           0.75     19008\n",
      "   macro avg       0.84      0.66      0.66     19008\n",
      "weighted avg       0.81      0.75      0.71     19008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "cv_scores = cross_val_score(mnb, X_train, y_train, cv=10)\n",
    "params = {'alpha': [1.0, 1.25, 1.35, 1.4]}\n",
    "nb_grid = GridSearchCV(mnb, params, cv=5) #using grid search for hyperparameter tuning\n",
    "\n",
    "nb_grid.fit(X_train, y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, nb_grid.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.96      0.80      5215\n",
      "           1       0.74      0.19      0.30      2932\n",
      "\n",
      "    accuracy                           0.68      8147\n",
      "   macro avg       0.71      0.58      0.55      8147\n",
      "weighted avg       0.70      0.68      0.62      8147\n",
      "\n",
      "Recall: 0.1882673942701228\n"
     ]
    }
   ],
   "source": [
    "#tesing \n",
    "print(classification_report(y_test, nb_grid.predict(X_test),np.unique(y))) \n",
    "y_pred = nb_grid.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Decision Tree </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.74     12166\n",
      "           1       0.51      0.42      0.46      6842\n",
      "\n",
      "    accuracy                           0.65     19008\n",
      "   macro avg       0.61      0.60      0.60     19008\n",
      "weighted avg       0.63      0.65      0.64     19008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtree = tree.DecisionTreeClassifier(#max_depth=6,\n",
    "    class_weight=\"balanced\",\n",
    "    min_weight_fraction_leaf=0.03)\n",
    "dtree = dtree.fit(X_train,y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, dtree.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.76      0.73      5215\n",
      "           1       0.49      0.41      0.44      2932\n",
      "\n",
      "    accuracy                           0.63      8147\n",
      "   macro avg       0.59      0.58      0.58      8147\n",
      "weighted avg       0.62      0.63      0.62      8147\n",
      "\n",
      "Recall: 0.4051841746248295\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, dtree.predict(X_test),np.unique(y))) #testing\n",
    "y_pred = dtree.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Random Forest </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78     12166\n",
      "           1       0.61      0.56      0.58      6842\n",
      "\n",
      "    accuracy                           0.71     19008\n",
      "   macro avg       0.68      0.68      0.68     19008\n",
      "weighted avg       0.71      0.71      0.71     19008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400,\n",
    "                            max_features='sqrt',\n",
    "                            class_weight='balanced',\n",
    "                            max_depth=5,\n",
    "                            oob_score=True,\n",
    "                            random_state=3,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, rf.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.77      0.75      5215\n",
      "           1       0.56      0.51      0.53      2932\n",
      "\n",
      "    accuracy                           0.68      8147\n",
      "   macro avg       0.65      0.64      0.64      8147\n",
      "weighted avg       0.67      0.68      0.67      8147\n",
      "\n",
      "Recall: 0.5109140518417462\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, rf.predict(X_test), np.unique(y))) \n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> SVM (Balanced) </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.77      0.81     12166\n",
      "           1       0.65      0.77      0.70      6842\n",
      "\n",
      "    accuracy                           0.77     19008\n",
      "   macro avg       0.75      0.77      0.76     19008\n",
      "weighted avg       0.78      0.77      0.77     19008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(C=0.05, max_iter=5000, class_weight='balanced', random_state=3)\n",
    "svm.fit(X_train,y_train)\n",
    "#training\n",
    "print(classification_report(y_train, svm.predict(X_train), np.unique(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74      5215\n",
      "           1       0.55      0.63      0.59      2932\n",
      "\n",
      "    accuracy                           0.68      8147\n",
      "   macro avg       0.66      0.67      0.66      8147\n",
      "weighted avg       0.69      0.68      0.69      8147\n",
      "\n",
      "Recall: 0.633356070941337\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, svm.predict(X_test), np.unique(y))) \n",
    "y_pred = svm.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> SVM (Unbalanced) </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.50      0.64     12166\n",
      "           1       0.50      0.91      0.65      6842\n",
      "\n",
      "    accuracy                           0.65     19008\n",
      "   macro avg       0.71      0.70      0.65     19008\n",
      "weighted avg       0.76      0.65      0.65     19008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(C=0.05, max_iter=5000, class_weight={0: 0.6, 1: 1.5}, random_state=3)\n",
    "svm.fit(X_train,y_train)\n",
    "#training\n",
    "print(classification_report(y_train, svm.predict(X_train), np.unique(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.43      0.57      5215\n",
      "           1       0.46      0.85      0.59      2932\n",
      "\n",
      "    accuracy                           0.58      8147\n",
      "   macro avg       0.65      0.64      0.58      8147\n",
      "weighted avg       0.70      0.58      0.58      8147\n",
      "\n",
      "Recall: 0.8543656207366985\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, svm.predict(X_test), np.unique(y))) \n",
    "y_pred = svm.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u> Logistic Regression </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.79      0.83     12166\n",
      "           1       0.68      0.79      0.73      6842\n",
      "\n",
      "    accuracy                           0.79     19008\n",
      "   macro avg       0.78      0.79      0.78     19008\n",
      "weighted avg       0.80      0.79      0.80     19008\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tatiksha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logis = LogisticRegression(class_weight = \"balanced\") \n",
    "logis.fit(X_train, y_train)\n",
    "\n",
    "#training\n",
    "print(classification_report(y_train, logis.predict(X_train), np.unique(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74      5215\n",
      "           1       0.55      0.62      0.58      2932\n",
      "\n",
      "    accuracy                           0.68      8147\n",
      "   macro avg       0.66      0.67      0.66      8147\n",
      "weighted avg       0.69      0.68      0.69      8147\n",
      "\n",
      "Recall: 0.6224420190995907\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "print(classification_report(y_test, logis.predict(X_test), np.unique(y))) \n",
    "y_pred = logis.predict(X_test)\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
